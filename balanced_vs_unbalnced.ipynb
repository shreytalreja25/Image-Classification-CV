{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, TensorDataset\n",
    "from torchvision import models, datasets, transforms\n",
    "from PIL import Image\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(source_dir, output_dir, train_ratio=0.8, seed=42, type='balanced'):\n",
    "    random.seed(seed)\n",
    "\n",
    "    categories = [cat for cat in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, cat))]\n",
    "    if type != 'balanced':\n",
    "        random.shuffle(categories)\n",
    "    summary = {}\n",
    "\n",
    "    print(f\"\\n📁 Splitting dataset from: {source_dir}\")\n",
    "    print(f\"💾 Output directory: {output_dir}\\n\")\n",
    "\n",
    "    for idx, category in enumerate(categories):\n",
    "        print(idx, category)\n",
    "        category_path = os.path.join(source_dir, category)\n",
    "        images = os.listdir(category_path)\n",
    "        random.shuffle(images)\n",
    "\n",
    "        split_idx = int(len(images) * train_ratio)\n",
    "\n",
    "        train_images = images[:split_idx]\n",
    "        test_images = images[split_idx:]\n",
    "\n",
    "        if type == 'unbalanced':\n",
    "            max_train_images = split_idx - (idx * 35)\n",
    "            train_images = train_images[:max_train_images]\n",
    "            print(len(train_images), max_train_images)\n",
    "\n",
    "        summary[category] = {\n",
    "            'train': len(train_images),\n",
    "            'test': len(test_images)\n",
    "        }\n",
    "\n",
    "        print(f\"\\n🔹 Splitting category: {category}\")\n",
    "        for split_name, split_images in [('train', train_images), ('test', test_images)]:\n",
    "            split_path = Path(output_dir) / split_name / category\n",
    "            split_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for img in tqdm(split_images, desc=f\"   ➤ {split_name.upper()} [{category}]\", ncols=80):\n",
    "                src = Path(category_path) / img\n",
    "                dst = split_path / img\n",
    "                try:\n",
    "                    shutil.copyfile(src, dst)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Could not copy {img}: {e}\")\n",
    "\n",
    "    # Final Summary\n",
    "    print(\"\\n📊 Split Summary:\")\n",
    "    total_train, total_test = 0, 0\n",
    "    for category, stats in summary.items():\n",
    "        print(f\"   {category}: {stats['train']} train / {stats['test']} test\")\n",
    "        total_train += stats['train']\n",
    "        total_test += stats['test']\n",
    "\n",
    "    print(\"\\n✅ All categories processed successfully!\")\n",
    "    print(f\"🗂️  Total training images: {total_train}\")\n",
    "    print(f\"🧪 Total testing images:  {total_test}\")\n",
    "    print(f\"\\n📍 Check your data at: {output_dir}/train/ and {output_dir}/test/\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred, class_names=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance and optionally print a report.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision_macro\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n📊 Classification Report:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, normalize=False, figsize=(10, 8), title=\"Confusion Matrix\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and optionally save a confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"📁 Saved confusion matrix to {save_path}\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features(image_path, max_descriptors=500):\n",
    "    try:\n",
    "        img = cv2.imread(image_path, 0)\n",
    "        sift = cv2.SIFT_create(nfeatures=max_descriptors)\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        if descriptors is None:\n",
    "            return np.zeros((max_descriptors, 128)).flatten()\n",
    "\n",
    "        if descriptors.shape[0] > max_descriptors:\n",
    "            descriptors = descriptors[:max_descriptors]\n",
    "        else:\n",
    "            pad = np.zeros((max_descriptors - descriptors.shape[0], 128))\n",
    "            descriptors = np.vstack((descriptors, pad))\n",
    "\n",
    "        return descriptors.flatten()\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_dir, max_descriptors=500, use_pca=True, pca_dim=300, type='balanced'):\n",
    "    cache_name = f\"{'pca' if use_pca else 'nopca'}_{os.path.basename(data_dir)}_sift_{max_descriptors}.npz\"\n",
    "    cache_path = os.path.join(CACHE_DIR, type, cache_name)\n",
    "    os.makedirs(f\"{CACHE_DIR}/{type}\", exist_ok=True)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"\\nLoading cached features from {cache_path}\")\n",
    "        cache = np.load(cache_path, allow_pickle=True)\n",
    "        return cache[\"X\"], cache[\"y\"], cache[\"labels\"].tolist(), cache[\"pca\"] if \"pca\" in cache.files else None\n",
    "\n",
    "    print(f\"\\nExtracting features from: {data_dir}\")\n",
    "    X, y = [], []\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label)\n",
    "        if not os.path.isdir(label_path): continue\n",
    "\n",
    "        files = os.listdir(label_path)\n",
    "        print(f\"{label} - {len(files)} images\")\n",
    "\n",
    "        for file in tqdm(files, desc=f\"   Extracting [{label}]\", ncols=80):\n",
    "            image_path = os.path.join(label_path, file)\n",
    "            feat = extract_sift_features(image_path, max_descriptors)\n",
    "            if feat is not None:\n",
    "                X.append(feat)\n",
    "                y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    labels = label_encoder.classes_.tolist()\n",
    "\n",
    "    pca_obj = None\n",
    "    if use_pca:\n",
    "        print(\"Applying PCA to reduce dimensionality...\")\n",
    "        pca_obj = PCA(n_components=pca_dim)\n",
    "        # pca_obj = PCA()\n",
    "        X = pca_obj.fit_transform(X)\n",
    "\n",
    "    print(f\"Caching features to {cache_path}\")\n",
    "    np.savez_compressed(cache_path, X=X, y=y, labels=labels, pca=pca_obj)\n",
    "\n",
    "    return X, y, labels, pca_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train, type='balanced'):\n",
    "    print(\"\\nTraining SVM classifier (SIFT + SVM)... This may take a few minutes.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    clf = SVC(kernel='linear', probability=True, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Training complete. Time taken: {duration:.2f} seconds.\")\n",
    "\n",
    "    # Save model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(f\"models/{type}\", exist_ok=True)\n",
    "    model_path = f\"models/{type}/svm_classifier.joblib\"\n",
    "    joblib.dump(clf, model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(X_train, y_train, type='balanced'):\n",
    "    print(\"\\nTraining SGDClassifier (Linear SVM approximation)...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    clf = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Training complete. Time taken: {duration:.2f} seconds.\")\n",
    "\n",
    "    # Save the model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(f\"models/{type}\", exist_ok=True)\n",
    "    model_path = f\"models/{type}/sgd_classifier.joblib\"\n",
    "    joblib.dump(clf, model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(X_train, y_train, type='balanced'):\n",
    "    print(\"\\nTraining Random Forest classifier...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Training complete. Time taken: {duration:.2f} seconds.\")\n",
    "    \n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(f\"models/{type}\", exist_ok=True)\n",
    "    model_path = f\"models/{type}/sgd_classifier.joblib\"\n",
    "    joblib.dump(clf, model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X_test, y_test, labels, method, type='balanced'):\n",
    "    print(\"\\nEvaluating model...\")\n",
    "\n",
    "    preds = clf.predict(X_test)\n",
    "    report = classification_report(y_test, preds, target_names=labels, digits=4)\n",
    "    metrics = evaluate_classification(y_test, preds, labels, verbose=False)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    result_dir = os.path.join(\"results\", type, \"ML_results\", f\"{method}_{timestamp}\")\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{result_dir}/{type}\", exist_ok=True)\n",
    "\n",
    "    report_file = os.path.join(result_dir, type, \"report.txt\")\n",
    "    with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Evaluation Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Model: SIFT + {method}\\n\")\n",
    "        f.write(\"\\nSummary Metrics:\\n\")\n",
    "        for k, v in metrics.items():\n",
    "            f.write(f\"  {k}: {v:.4f}\\n\")\n",
    "        f.write(\"\\nFull Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Report saved to: {report_file}\")\n",
    "\n",
    "    cm_path = os.path.join(result_dir, \"confmat.png\")\n",
    "    plot_confusion_matrix(\n",
    "        y_true=y_test,\n",
    "        y_pred=preds,\n",
    "        class_names=labels,\n",
    "        normalize=True,\n",
    "        title=f\"SIFT + {method} Confusion Matrix\",\n",
    "        save_path=cm_path\n",
    "    )\n",
    "    print(f\"Confusion matrix saved to: {cm_path}\")\n",
    "\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(dataset):\n",
    "    counts = Counter(dataset.targets)\n",
    "    num_samples = sum(counts.values())\n",
    "    weights = [num_samples / counts[i] for i in range(len(counts))]\n",
    "    return torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(data_dir, batch_size=32, type=\"balanced\"):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "\n",
    "    train_ds = datasets.ImageFolder(os.path.join(data_dir, type, 'train'), transform=train_transform)\n",
    "    test_ds = datasets.ImageFolder(os.path.join(data_dir, type, 'test'), transform=test_transform)\n",
    "\n",
    "\n",
    "    if type != 'balanced':\n",
    "        # Count instances of each class\n",
    "        class_counts = Counter(train_ds.targets)\n",
    "        class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "        sample_weights = [class_weights[label] for label in train_ds.targets]\n",
    "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_efficientnet(train_loader, num_classes, num_epochs=5, lr=0.001, type='balanced', class_weights=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "    in_features = model._fc.in_features\n",
    "    model._fc = nn.Linear(in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=100)\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\n✅ Epoch {epoch+1} completed — Avg Loss: {avg_loss:.4f} ⏱️ {epoch_time:.2f}s\")\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(f\"models/{type}\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"models/{type}/efficientnet_b0.pth\")\n",
    "    print(f\"Model saved to: models/{type}/efficientnet_b0.pth\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(train_loader, num_classes, num_epochs=5, lr=0.001, type='balanced', class_weights=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_name = torch.cuda.get_device_name(0) if device.type == \"cuda\" else \"CPU\"\n",
    "    print(f\"\\n🖥️  Training on: {device} ({device_name})\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"🧠 CUDA Memory Allocated: {torch.cuda.memory_allocated() // (1024 ** 2)} MB\")\n",
    "\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # freeze feature extractor\n",
    "\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.fc.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"🔁 Epoch {epoch+1}/{num_epochs}\", ncols=100)\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\n✅ Epoch {epoch+1} completed — Avg Loss: {avg_loss:.4f} ⏱️ {epoch_time:.2f}s\")\n",
    "\n",
    "    # Save model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(f\"models/{type}\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"models/{type}/resnet18_finetuned.pth\")\n",
    "    print(f\"💾 Model saved to: models/{type}/resnet18_finetuned.pth\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mobilenet(train_loader, num_classes, num_epochs=5, lr=0.001, type='balanced', class_weights=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "    model = models.mobilenet_v2(pretrained=True)\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False  # freeze backbone\n",
    "\n",
    "    model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=100)\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\n✅ Epoch {epoch+1} completed — Avg Loss: {avg_loss:.4f} ⏱️ {epoch_time:.2f}s\")\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(f\"models/{type}\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), f\"models/{type}/mobilenet_v2.pth\")\n",
    "    print(f\"Model saved to: models/{type}/mobilenet_v2.pth\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, class_names, method, type='balanced'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
    "    metrics = evaluate_classification(all_labels, all_preds, class_names, verbose=False)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "    result_dir = os.path.join(\"results\", type, \"DL_results\", f\"{method}_eval_{timestamp}\")\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    report_file = os.path.join(result_dir, \"report.txt\")\n",
    "    with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Evaluation Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Model: {method}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\\n\")\n",
    "        f.write(\"Summary Metrics:\\n\")\n",
    "        for k, v in metrics.items():\n",
    "            f.write(f\"  {k}: {v:.4f}\\n\")\n",
    "        f.write(\"\\nFull Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "    print(f\"Report saved to: {report_file}\")\n",
    "\n",
    "    cm_path = os.path.join(result_dir, \"confmat.png\")\n",
    "    plot_confusion_matrix(\n",
    "        y_true=all_labels,\n",
    "        y_pred=all_preds,\n",
    "        class_names=class_names,\n",
    "        normalize=True,\n",
    "        title=f\"{method} Confusion Matrix\",\n",
    "        save_path=cm_path\n",
    "    )\n",
    "\n",
    "    print(f\"Confusion matrix saved to: {cm_path}\")\n",
    "    print(f\"\\nClassification Report ({method}):\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_methods(type='balanced'):\n",
    "    # split_dataset('archive/Aerial_Landscapes', f\"data/{type}\", type=type)\n",
    "    # X_train, y_train, class_names, _ = prepare_data(f\"data/{type}/train\", use_pca=True, type=type)\n",
    "    # X_test, y_test, _, _ = prepare_data(f\"data/{type}/test\", use_pca=True, type=type)\n",
    "\n",
    "    # clf_sgd = train_sgd(X_train, y_train, type=type)\n",
    "    # clf_svm = train_svm(X_train, y_train, type=type)\n",
    "    # clf_rf = train_rf(X_train, y_train, type=type)\n",
    "    \n",
    "    # evaluate(clf_sgd, X_test, y_test, class_names, 'SGD', type=type)\n",
    "    # evaluate(clf_svm, X_test, y_test, class_names, 'SVM', type=type)\n",
    "    # evaluate(clf_rf, X_test, y_test, class_names, 'RandomForest', type=type)\n",
    "\n",
    "\n",
    "    train_loader, test_loader, class_names = get_data_loaders('data', batch_size=32)\n",
    "    if type != 'balanced':\n",
    "        weights = get_class_weights(train_loader.dataset)\n",
    "\n",
    "    model_efficientnet = train_efficientnet(train_loader, num_classes=len(class_names), num_epochs=5, type=type, class_weights=weights)\n",
    "    # model_resnet = train_resnet(train_loader, num_classes=len(class_names), num_epochs=5, type=type, class_weights=weights)\n",
    "    # model_mobilenet = train_mobilenet(train_loader, num_classes=len(class_names), num_epochs=5, type=type, class_weights=weights)\n",
    "    \n",
    "    evaluate_model(model_efficientnet, test_loader, class_names, 'Efficientnet', type=type)\n",
    "    # evaluate_model(model_resnet, test_loader, class_names, 'Resnet', type=type)\n",
    "    # evaluate_model(model_mobilenet, test_loader, class_names, 'Mobilenet', type=type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Splitting dataset from: archive/Aerial_Landscapes\n",
      "💾 Output directory: data/balanced\n",
      "\n",
      "\n",
      "🔹 Splitting category: Agriculture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Agriculture]: 100%|█████████████| 640/640 [00:00<00:00, 1672.08it/s]\n",
      "   ➤ TEST [Agriculture]: 100%|██████████████| 160/160 [00:00<00:00, 1697.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Airport\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Airport]: 100%|█████████████████| 640/640 [00:00<00:00, 1704.97it/s]\n",
      "   ➤ TEST [Airport]: 100%|██████████████████| 160/160 [00:00<00:00, 1940.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Beach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Beach]: 100%|███████████████████| 640/640 [00:00<00:00, 1966.35it/s]\n",
      "   ➤ TEST [Beach]: 100%|████████████████████| 160/160 [00:00<00:00, 1786.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: City\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [City]: 100%|████████████████████| 640/640 [00:00<00:00, 1171.41it/s]\n",
      "   ➤ TEST [City]: 100%|█████████████████████| 160/160 [00:00<00:00, 1490.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Desert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Desert]: 100%|██████████████████| 640/640 [00:00<00:00, 1769.28it/s]\n",
      "   ➤ TEST [Desert]: 100%|███████████████████| 160/160 [00:00<00:00, 1987.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Forest]: 100%|██████████████████| 640/640 [00:00<00:00, 1434.74it/s]\n",
      "   ➤ TEST [Forest]: 100%|███████████████████| 160/160 [00:00<00:00, 1521.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Grassland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Grassland]: 100%|███████████████| 640/640 [00:00<00:00, 1397.00it/s]\n",
      "   ➤ TEST [Grassland]: 100%|████████████████| 160/160 [00:00<00:00, 1185.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Highway\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Highway]: 100%|█████████████████| 640/640 [00:00<00:00, 1613.73it/s]\n",
      "   ➤ TEST [Highway]: 100%|██████████████████| 160/160 [00:00<00:00, 1090.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Lake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Lake]: 100%|████████████████████| 640/640 [00:00<00:00, 1367.81it/s]\n",
      "   ➤ TEST [Lake]: 100%|█████████████████████| 160/160 [00:00<00:00, 1876.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Mountain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Mountain]: 100%|████████████████| 640/640 [00:00<00:00, 1223.08it/s]\n",
      "   ➤ TEST [Mountain]: 100%|█████████████████| 160/160 [00:00<00:00, 1066.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Parking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Parking]: 100%|█████████████████| 640/640 [00:00<00:00, 1054.06it/s]\n",
      "   ➤ TEST [Parking]: 100%|███████████████████| 160/160 [00:00<00:00, 741.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Port\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Port]: 100%|█████████████████████| 640/640 [00:00<00:00, 774.81it/s]\n",
      "   ➤ TEST [Port]: 100%|██████████████████████| 160/160 [00:00<00:00, 677.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Railway\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Railway]: 100%|██████████████████| 640/640 [00:01<00:00, 608.59it/s]\n",
      "   ➤ TEST [Railway]: 100%|███████████████████| 160/160 [00:00<00:00, 721.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: Residential\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [Residential]: 100%|██████████████| 640/640 [00:01<00:00, 611.70it/s]\n",
      "   ➤ TEST [Residential]: 100%|███████████████| 160/160 [00:00<00:00, 665.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Splitting category: River\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   ➤ TRAIN [River]: 100%|████████████████████| 640/640 [00:01<00:00, 630.42it/s]\n",
      "   ➤ TEST [River]: 100%|█████████████████████| 160/160 [00:00<00:00, 613.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Split Summary:\n",
      "   Agriculture: 640 train / 160 test\n",
      "   Airport: 640 train / 160 test\n",
      "   Beach: 640 train / 160 test\n",
      "   City: 640 train / 160 test\n",
      "   Desert: 640 train / 160 test\n",
      "   Forest: 640 train / 160 test\n",
      "   Grassland: 640 train / 160 test\n",
      "   Highway: 640 train / 160 test\n",
      "   Lake: 640 train / 160 test\n",
      "   Mountain: 640 train / 160 test\n",
      "   Parking: 640 train / 160 test\n",
      "   Port: 640 train / 160 test\n",
      "   Railway: 640 train / 160 test\n",
      "   Residential: 640 train / 160 test\n",
      "   River: 640 train / 160 test\n",
      "\n",
      "✅ All categories processed successfully!\n",
      "🗂️  Total training images: 9600\n",
      "🧪 Total testing images:  2400\n",
      "\n",
      "📍 Check your data at: data/balanced/train/ and data/balanced/test/\n",
      "\n",
      "\n",
      "Extracting features from: data/balanced/train\n",
      "Agriculture - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Agriculture]: 100%|████████████| 640/640 [00:24<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Airport]: 100%|████████████████| 640/640 [00:27<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beach - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Beach]: 100%|██████████████████| 640/640 [00:24<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [City]: 100%|███████████████████| 640/640 [00:28<00:00, 22.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desert - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Desert]: 100%|█████████████████| 640/640 [00:26<00:00, 23.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Forest]: 100%|█████████████████| 640/640 [00:37<00:00, 17.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grassland - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Grassland]: 100%|██████████████| 640/640 [00:31<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highway - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Highway]: 100%|████████████████| 640/640 [00:45<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Lake]: 100%|███████████████████| 640/640 [00:44<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mountain - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Mountain]: 100%|███████████████| 640/640 [01:00<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parking - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Parking]: 100%|████████████████| 640/640 [01:10<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Port - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Port]: 100%|███████████████████| 640/640 [10:41<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Railway - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Railway]: 100%|████████████████| 640/640 [01:40<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residential - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [Residential]: 100%|████████████| 640/640 [11:13<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "River - 640 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Extracting [River]: 100%|██████████████████| 640/640 [01:53<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA to reduce dimensionality...\n",
      "Caching features to cache\\balanced\\pca_train_sift_500.npz\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cache\\\\balanced\\\\pca_train_sift_500.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m, in \u001b[0;36mrun_methods\u001b[1;34m(type)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_methods\u001b[39m(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     split_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marchive/Aerial_Landscapes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     X_train, y_train, encoder, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_pca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     X_test, y_test, _, _ \u001b[38;5;241m=\u001b[39m prepare_data(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_pca\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# clf_sgd = train_sgd(X_train, y_train, type=type)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# clf_svm = train_svm(X_train, y_train, type=type)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# clf_rf = train_rf(X_train, y_train, type=type)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# evaluate(clf_svm, X_test, y_test, encoder, 'SVM', type=type)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# evaluate(clf_rf, X_test, y_test, encoder, 'RandomForest', type=type)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(data_dir, max_descriptors, use_pca, pca_dim, type)\u001b[0m\n\u001b[0;32m     37\u001b[0m     X \u001b[38;5;241m=\u001b[39m pca_obj\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaching features to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavez_compressed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpca_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, labels, pca_obj\n",
      "File \u001b[1;32mc:\\Users\\deepesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:763\u001b[0m, in \u001b[0;36msavez_compressed\u001b[1;34m(file, allow_pickle, *args, **kwds)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_savez_compressed_dispatcher)\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavez_compressed\u001b[39m(file, \u001b[38;5;241m*\u001b[39margs, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    693\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m    Save several arrays into a single file in compressed ``.npz`` format.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m \n\u001b[0;32m    762\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 763\u001b[0m     \u001b[43m_savez\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:789\u001b[0m, in \u001b[0;36m_savez\u001b[1;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    787\u001b[0m     compression \u001b[38;5;241m=\u001b[39m zipfile\u001b[38;5;241m.\u001b[39mZIP_STORED\n\u001b[1;32m--> 789\u001b[0m zipf \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m namedict\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\deepesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:103\u001b[0m, in \u001b[0;36mzipfile_factory\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m    102\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowZip64\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cache\\\\balanced\\\\pca_train_sift_500.npz'"
     ]
    }
   ],
   "source": [
    "run_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading cached features from cache\\unbalanced\\pca_train_sift_500.npz\n",
      "\n",
      "Loading cached features from cache\\unbalanced\\pca_test_sift_500.npz\n",
      "\n",
      "Using device: cpu\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  21%|███████▊                              | 62/300 [06:25<37:25,  9.43s/it, Loss=0.4557]"
     ]
    }
   ],
   "source": [
    "run_methods('unbalanced')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
